#VM 생성 및 Resource Group 등 생성
----------------------------------------------------------------------------------------------
리소스그룹 : jdwon-rsgrp
ACR   이름 : jdwon
AKS   이름 : jdwon-aks     (node 갯수=4)
----------------------------------------------------------------------------------------------

# AKS 접속 및 ACR 연결(azure shell에서 작업)
-----------------------------------------------------------------------------------------------
as login   => msaez 환경에서 Azure 포탈 접속시에만 해당(Azure Shell에서는 필요없음)
az aks get-credentials --resource-group jdwon-rsgrp --name jdwon-aks
az aks update -n jdwon-aks -g jdwon-rsgrp --attach-acr jdwon
-----------------------------------------------------------------------------------------------

# Helm 및 kafka 설치
----------------------------------------------------------------------------------------------
helm version     => 버전확인(3.4 이상이어야 함)
kubectl create ns kafka    => Namespace 생성
kubectl --namespace kube-system create sa tiller
kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
helm repo add incubator https://charts.helm.sh/incubator
helm repo update
helm install my-kafka --namespace kafka incubator/kafka
----------------------------------------------------------------------------------------------

# Httpie 설치
----------------------------------------------------------------------------------------------
cat <<EOF | kubectl apply -f -
apiVersion: "v1"
kind: "Pod"
metadata:
  name: httpie
  labels:
    name: httpie
spec:
  containers:
    - name: httpie
      image: clue/httpie
      command:
        - sleep
        - "360000"
EOF
----------------------------------------------------------------------------------------------

# Git Hub에서 소스코드 가져옴
----------------------------------------------------------------------------------------------
git clone https://github.com/jongdukwon/sk2.git
----------------------------------------------------------------------------------------------


# 소스코드 빌드/도커라이징(이미지 생성 및 Azrue 레지스트리에 Push) => 각각의 서비스 디렉토리로 이동하여 실행
----------------------------------------------------------------------------------------------
cd reservation
mvn package   
az acr build --registry jdwon --image jdwon.azurecr.io/reservation:latest . 
cd ../deposit
mvn package   
az acr build --registry jdwon --image jdwon.azurecr.io/deposit:latest . 
cd ../restaurant
mvn package   
az acr build --registry jdwon --image jdwon.azurecr.io/restaurant:latest . 
cd ../customercenter
mvn package   
az acr build --registry jdwon --image jdwon.azurecr.io/customercenter:latest . 
cd ../gateway
mvn package   
az acr build --registry jdwon --image jdwon.azurecr.io/gateway:latest . 
----------------------------------------------------------------------------------------------

kubectl create ns sk2    => Namespace 생성

# 컨테이너라이징 : Deployment,Pod,Replica 및 Service 생성 / sk2 Namespace에 배포함 (sk2 directory로 이동)
----------------------------------------------------------------------------------------------          
# CASE1> Yaml 방식 (gateway는 Case2 API방식으로 추가 배포)
kubeclt apply -f sk2-deployment.yaml -n sk2

# CASE2 > API 방식
kubectl create deploy reservation --image=jdwon.azurecr.io/reservation:latest -n sk2
kubectl expose deploy reservation --type="ClusterIP" --port=8080 -n sk2
kubectl create deploy deposit --image=jdwon.azurecr.io/deposit:latest -n sk2
kubectl expose deploy deposit --type="ClusterIP" --port=8080 -n sk2
kubectl create deploy restaurant --image=jdwon.azurecr.io/restaurant:latest -n sk2
kubectl expose deploy restaurant --type="ClusterIP" --port=8080 -n sk2
kubectl create deploy customercenter --image=jdwon.azurecr.io/customercenter:latest -n sk2
kubectl expose deploy customercenter --type="ClusterIP" --port=8080 -n sk2

kubectl create deploy gateway --image=jdwon.azurecr.io/gateway:latest -n sk2
kubectl expose deploy gateway --type=LoadBalancer --port=8080 -n sk2
----------------------------------------------------------------------------------------------      

# Kafka Topic 생성
----------------------------------------------------------------------------------------------  
kubectl -n kafka exec my-kafka-0 -- /usr/bin/kafka-topics --zookeeper my-kafka-zookeeper:2181 --topic restaurant --create --partitions 1 --replication-factor 1
kubectl -n kafka exec my-kafka-0 -- /usr/bin/kafka-topics --zookeeper my-kafka-zookeeper:2181 --list                       <==생성한 Topic 확인
kubectl -n kafka exec my-kafka-0 -- /usr/bin/kafka-topics --zookeeper my-kafka-zookeeper:2181 --topic restaurant --delete  <= Topic 삭제(필요한 경우에만 사용)
----------------------------------------------------------------------------------------------  

# Kafka 이벤트 수신
----------------------------------------------------------------------------------------------  
kubectl -n kafka exec -ti my-kafka-0 -- /usr/bin/kafka-console-consumer --bootstrap-server my-kafka:9092 --topic restaurant --from-beginning
----------------------------------------------------------------------------------------------  


# 확인(geteway service의 IP를 먼저 확인해 둘 것: ClusterIP, External_IP 상관없음)
---------------------------------------------------------------------------------------------- 
kubectl exec -it httpie -- bin/bash                   <= Httpie 실행
root@httpie:/# http POST http://10.0.248.223:8080/reservations restaurantNo=10 day=20140301
-------------------------------
>>결과화면 Sample
HTTP/1.1 201 Created
Content-Type: application/json;charset=UTF-8
Date: Mon, 08 Feb 2021 14:05:20 GMT
Location: http://reservation:8080/reservations/4
transfer-encoding: chunked

{
    "_links": {
        "reservation": {
            "href": "http://reservation:8080/reservations/4"
        },
        "self": {
            "href": "http://reservation:8080/reservations/4"
        }
    },
    "day": "20140301",
    "restaurantNo": "10",
    "status": "DepositPayed"
}
-------------------------------------------------------------------------------------------

# Siege 배포
-------------------------------------------------------------------------------------------
kubectl run siege --image=apexacme/siege-nginx -n sk2
kubectl get pod -n sk2   => siege 생성여부 확인
-------------------------------------------------------------------------------------------

#### 7. Circuit Breaker(skip)
-------------------------------------------------------------------------------------------
#siege 들어가기:
kubectl exec -it pod/siege-5459b87f86-jqk76 -c siege -n sk2 -- /bin/bash


=> 테스트 중
#Siege에서 동시사용자 100명 30초동안 부하발생
siege -c100 -t30S -r10 -v --content-type "application/json" 'http://10.0.92.167:8080/reservations POST {"restaurantNo": "10", "day":"20210214"}'
-------------------------------------------------------------------------------------------

#### 8. Autoscale (HPA)
-------------------------------------------------------------------------------------------
#예약 시스템에 대한 replica 를 동적으로 늘려주도록 HPA 를 설정한다. 
#CPU 사용량이 15프로를 넘어서면 replica 를 10개까지 늘려준다

kubectl autoscale deploy reservation --min=1 --max=10 --cpu-percent=15 -n sk2

#siege 들어가기:
kubectl exec -it pod/siege-5459b87f86-jqk76 -c siege -n sk2 -- /bin/bash
#Siege에서 동시사용자 100명 30초동안 부하발생
siege -c100 -t30S -r10 -v --content-type "application/json" 'http://10.0.92.167:8080/reservations POST {"restaurantNo": "10", "day":"20210214"}'

#부하가 증가함에 따라 reservation Pod가 점차 증가하는것이 확인된다. 
#부하가 감소하면 늘어난 Scale이 점차 감소한다.
#watch kubectl get po -n sk2
-------------------------------------------------------------------------------------------

9.Zero-downtime deploy (Readiness Probe)
-------------------------------------------------------------------------------------------
#현재 Radiness Probe가 적용된 상태임
#새로운 Reservation을 새로생성한 이미지로 교체한다.
az acr build --registry jdwon --image jdwon.azurecr.io/reservation:r1 .
kubectl set image deploy reservation reservation=jdwon.azurecr.io/reservation:r1 -n sk2
#새로운 reservation pod가 생성된 이후에 기존의 reservation가 자동으로 삭제된다. 
-------------------------------------------------------------------------------------------

10.Config Map/ Persistence Volume
-------------------------------------------------------------------------------------------


-------------------------------------------------------------------------------------------

11.Polyglot
-------------------------------------------------------------------------------------------


-------------------------------------------------------------------------------------------

12.Self-healing (Liveness Probe)
-------------------------------------------------------------------------------------------
#현재 Liveness Porbe가 적용된 상태임
#테스트를 위해 Liveness Probe 체크 Port를 변경하여 Reservation을 Deploy 하였다.
kubectl delete deploy reservation -n sk2
kubectl apply -f 12-SelfHealing.yaml -n sk2

#Liveness Porbe 체크 오류로 인해 Reservation Pod가 Restart됨을 학인할 수 있다.
-------------------------------------------------------------------------------------------


1.Saga
2.CQRS
3.Correlation
4.Req/Resp
5.Gateway
6.Deploy/ Pipeline
7.Circuit Breaker



